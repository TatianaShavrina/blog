---
layout: post
title: Why do my keras text generation results do not reproduce?
comments: true
description: >
  Pathing frequent errors in text generation with Keras
tags: [NLP, keras, deep learning, text generation, python]
---

Buildind a simple and nice text generator in Keras is not a difficult task, yet there are a few mistakes in the framework, that prevent you from succeeding.
Today we will discuss a most popular example of an LSTM in Python, written by Trung Tran. [In his post](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/), he provides a simple architecture of a 2-layered char LSTM, that can learn rather fast and reproduce simple phrases:
> “Albus Dumbledore, I should, do you? But he doesn’t want to adding the thing that you are at Hogwarts, so we can run and get more than one else, you see you, Harry.”
> “What about this thing, you shouldn’t,” Harry said to Ron and Hermione. “I have no furious test,” said Hermione in a small voice.
> “Well, you can’t be the baby way?” said Harry. “He was a great Beater, he didn’t want to ask for more time.”

The main problem with this code is...the rsulting model is not reproducing its results after saving!
![](https://d3ebicv0uqgr7t.cloudfront.net/images/tarsier.png)

Later on, a very similar architecture was added as an official example [in Keras repository](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py).
There are a lot of errors opened, discussing the model seems "untrained":
[one]() [two]() [three]()


The original code can be found [on github](https://github.com/ChunML/text-generator/), but here I will provide both the code and my own vision on how to fix the errors.

## Fixing reproducibility


## Adding more info
When providing little  data to a generator like this one (and by little data I mean you are providing a few books, not a big webcorpus), in fact, you are waiting for your model to overfit. This way it can learn to reproduce words from the texts, but not to produce new ones, unless you are working with an [agglutinative language](https://en.wikipedia.org/wiki/Agglutinative_language) (English is not one of those!).
![](https://blog.datasciencedojo.com/wp-content/uploads/2016/04/generate-3.png)



## Further tuning
The better you now your data the better is the model. As usual in deep learning, you should check the quality of the model after every N iterations - for example, check the generated output after every 100 epochs:

```python
if nb_epoch % 10 == 0:
    generate_text(model, 20, VOCAB_SIZE, ix_to_char)
```
You can adjust the length of the context your model is looking at - in English, the basic parameter value is from 40 to 80 symbols.
![](https://cdn-images-1.medium.com/max/1600/1*gCWUibmQ8rszKxI3G19KmA.jpeg)

## Primary results
With this architecture, a really human-like results can be achieved on a small data. I tried to make a simple Telegram-bot, which generates proverbs "of different cultures" - Armenian, Indian, Sufi, Hasidic and Jewish (all in Russian) - you can find all the source code [in my repository](https://github.com/TatianaShavrina/NeuroBasnya/). 

Have fun!
