---
layout: post
title: An exhaustive list of open-source corpora for Russian
comments: true
description: >
  All projects for Russian with open source texts
tags: [NLP, machine learning, data, open source, Russian]
---

During my work as an NLP-engineer, I always encountered a lot of corpus projects, that are not so publicly well-known and mentioned, yet they are a good source of text data for different kinds of research.
Here I share this list with you, not forgetting to include more popular projects in it, of course, so that the list was complete.

Send your pull-requests [to this post](https://github.com/TatianaShavrina/blog/blob/master/_posts/2018-08-30-datasets.md) or comment below, if you know any dataset or corpus of the Russian language, which is not mentioned here!

# Table of contents
1. [Big and Open](#introduction)
2. [Special Corpora](#paragraph1)
    1. [Morphology and Syntax](#subparagraph1)
    2. [NER Parsing](#subparagraph2)
    3. [Spellcheckers](#subparagraph3)
    4. [Sentiment Analisys](#subparagraph4)
3. [Open Corpus Derivatives](#paragraph2)
    1. [Vector Models](#subparagraph5)
    2. [N-grams](#subparagraph6)


![](https://github.com/TatianaShavrina/blog/blob/master/assets/img/SsfrZojfM8hw4ckjn4K_EDl72eJkfbmt4t8yenImKBVvK0kTmF0xjctABnaLJIm9.jpeg)


# Big and Open <a name="introduction"></a>

* **Russian Twitter Corpus**

The corpus of short texts in Russian on the basis of  Twitter posts. 
Suitable for training a language model for social media or for short texts, training classifiers for sentiment analisys and text toxicity.

17,6 million tweets available!

[link](http://study.mokoron.com/)


* **Russian Common Crawl Data**

541 TB of raw text data from the web. Contains duplicates, sources  nd dates of the web-pages are non-obvious.

[link](https://wwwdb.inf.tu-dresden.de/misc/dwtc/)


* **Taiga Corpus**

_minute of shameless self-promotion_

Taiga corpus is a corpus project to become the largest fully available webcorpus constructed from open text sources. Data available on request, containing datasets for text classification, language modelling, fake news detection, thematic modelling, authorship attribution, social media research, etc. 

6,5 billion of tokens available

[link](https://github.com/TatianaShavrina/taiga_site)

---

# Special Corpora  <a name="paragraph1"></a>

## Morphology and Syntax  <a name="subparagraph1"></a>

* **OpenCorpora**

The first open-source corpus for Russian - about 2 million words in manual annotation available + dictionary

[link](https://nlpub.ru/OpenCorpora)

* **Russian National Corpus**

A subcorpus of Russian National Corpus is distributed openly by request.
Morphological annotation with manual verification.

1 million words available 
 
[link](http://ruscorpora.ru/corpora-usage.html)

* **General Internet-Corpus of Russian** 

LiveJournal and VKontakte corpus with the automatically resolved ambiguity
 - 2 million wordforms available
 
Annotation: Abbyy Compreno + rule-based verification
Available by request

[link](http://www.webcorpora.ru/silver)

* **MorphoRuEval Data**

Data for MorphoRuEval track - competition of automatic POS-tagging for Russian

Consists of:
 - plain texts:
1) LiveJournal (from GICR) 30 million words
2) Facebook, Twitter, VKontakte—30 million words
3) Librusec—300 million words
 - annotated data:
1) RNC Open: a manually disambiguated subcorpus of the Russian National Corpus—1.2 million words (fiction, news, nonfiction, spoken, blog)
2) GICR corpus with the resolved homonymy—1 million words
3) OpenCorpora.org data—400 thousand tokens
4) UD SynTagRus—900 thousand tokens (fiction, news)
 
[link](https://github.com/dialogue-evaluation/morphoRuEval-2017)

* **SynTagRus**

A subcorpus of Russian National Corpus, with fiction and news, with manual syntactic annotation.
Now converted to UD.
 - 1 million sentences available 
 
[link](https://github.com/UniversalDependencies/UD_Russian-SynTagRus)

* **Russian Universal Dependencies Data**

Russian texts with morphological and syntactic annotation in UD, checked manually

[SynTagRus, one more time](https://github.com/UniversalDependencies/UD_Russian-SynTagRus)

[GSD](http://universaldependencies.org/treebanks/ru_gsd/index.html)

[Parallel Universal Dependencies (PUD)](http://universaldependencies.org/treebanks/ru_pud/index.html)

[Taiga](http://universaldependencies.org/treebanks/ru_taiga/index.html)


## NER Parsing <a name="subparagraph2"></a>

* **FactRuEval Data**

Data from FactRuEval track for Russian - ORG, LOC, PER, LOCORG annotation with manual verification - from Lentapedia project and Wikinews

[link](https://github.com/dialogue-evaluation/factRuEval-2016)

* **Gareev Corpus**

A corpus compiled for different sources, training set for [Deepmipt Ner system](https://github.com/deepmipt/ner)

[link](https://link.springer.com/chapter/10.1007/978-3-642-37247-6_27)

Available by request, see paper:

Rinat Gareev, Maksim Tkachenko, Valery Solovyev, Andrey Simanovsky, Vladimir Ivanov: Introducing Baselines for Russian Named Entity Recognition. Computational Linguistics and Intelligent Text Processing, 329 -- 342 (2013). 

* **Persons-1000**

The collection of NER-tagged sentences annotated by experts of the Russian Academy of Sciences

[link](http://labinform.ru/pub/named_entities/descr_ne.htm)

* **Wikipedia Dumps**

Russian Wikipedia - full articles in different formats
 - 300 millions of tokens available
 
[link](http://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/)


* **DBpedia**

Structured Info fron Wikipedia in many languages, including Russian

[link](https://wiki.dbpedia.org/develop/datasets/dbpedia-version-2016-10)

RDF format downloads:

[link](https://wiki.dbpedia.org/downloads-2016-10#p10608-2)


## Spellcheckers <a name="subparagraph3"></a>

* **SpellRuEval trainset**

Dataset for SpellRuEval task for Russian - a track for spell-checkers for social media

10 000 sentences with errors from General Internet-Corpus of Russian

[link](http://www.dialog-21.ru/evaluation/2016/spelling_correction/)


## Sentiment Analysis <a name="subparagraph4"></a>

* **Russian Twitter Corpus** _(again)_

The corpus of short texts in Russian on the basis of  Twitter posts. 
Suitable for training a language model for social media or for short texts, training classifiers for sentiment analisys and text toxicity.

17,6 million tweets available for downloading!

[link](http://study.mokoron.com/)


* **SentiRuEval trainset**

Dataset from SentiRuEval task, 2016

about 20 000 tagged Tweets with manual verification
 
[link](https://drive.google.com/drive/u/0/folders/0BxlA8wH3PTUfV1F1UTBwVTJPd3c)


---

# Open Corpus Derivatives  <a name="paragraph2"></a>

## Vector Models <a name="subparagraph5"></a>

* **RusVectores Vector Models**

On RusVectores project you can download pretrained fasttext, word2vec and doc2vec models on the main Russian webcorpora:
(all available undec CC licence)

 - Russian National Corpus
 - Taiga
 - General Internet-Corpus
 - Aranea
 - News Corpus
 - Wikipedia
 
[link](http://rusvectores.org/ru/models/)

* **Word2vec**

Russian skip-gram model available for download resulting from the RUSSE evaluation track

[link](https://github.com/nlpub/russe-evaluation/tree/master/russe/measures/word2vec)


## Ngrams <a name="subparagraph6"></a>

* **Google Ngrams**

N-grams, calculated on Google Books data - available for multiple languages, Russian as well

[link](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)

* **Russian National Corpus Ngrams**

N-grams on Russian National Corpus, easy for downloading, top-100 also avaialble.

[link](http://www.ruscorpora.ru/corpora-freq.html)

* **Alexa Ngrams**

Data from top 10M domains of the web. More than 5 billion ngrams available on request for multiple languages:

[link](https://data.statoperator.com/)


* **Common Crawl Ngrams**

N-grams on Common Crawl Corpus, resulting from very noisy, yet big data from the web:

[link](http://statmt.org/ngrams/)





---




P.S. There is a great resource called [NLPub](https://nlpub.ru/%D0%A0%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B#.D0.9A.D0.BE.D1.80.D0.BF.D1.83.D1.81_.D1.82.D0.B5.D0.BA.D1.81.D1.82.D0.BE.D0.B2), where some of the resources are also listed.  
It is also good to add the other not listed resources there in wiki articles.
